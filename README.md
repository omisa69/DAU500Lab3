# DAU500Lab3

Overview
This project investigates the impact of three critical elements—learning rates, activation functions, and optimization algorithms—on the training and performance of deep neural networks. By conducting experiments on the MNIST, Fashion MNIST, and CIFAR10 datasets, it explores how these factors influence the efficiency and accuracy of neural network training. The assignment includes advanced techniques like learning rate tuning, activation function analysis, and optimization algorithm comparisons.

Objectives
The primary goals of this project are:

Learning Rate Tuning: Analyze how different learning rates affect convergence and model performance.
Activation Function Analysis: Compare various activation functions (e.g., ReLU, sigmoid, tanh) to understand their impact on network behavior and accuracy.
Optimization Algorithm Comparison: Evaluate the performance of different optimization algorithms, such as SGD, Adam, and RMSprop.
Dataset Exploration: Train and evaluate models on popular datasets like MNIST, Fashion MNIST, and CIFAR10, comparing results across the datasets.
